\documentclass[jou,apacite]{apa6}

\usepackage{amsmath}
\usepackage[]{natbib}

\newcommand{\titlesubtitle}[2]{\title{#1\\\vspace{2mm}\large #2}}

\titlesubtitle{Title}{9.66 Final Project}
\shorttitle{9.66 Final Project}

\author{Leon Lin}
\affiliation{\mbox{}}

\abstract{abstract}

\rightheader{What's this}
\leftheader{leon}

\newcommand{\Disj}{\text{Disj}}
\newcommand{\Conj}{\text{Conj}}
\newcommand{\True}{\text{True}}
\newcommand{\False}{\text{False}}

\begin{document}
\maketitle    
                        
Human minds have a striking ability to learn general concepts from
just a small number of examples. Two or perhaps just one example of,
say, a zebra or a wheel suffices to teach a child those respective
categories.

In this project we 
model the problem much as 
\citet*{rrdnf} do:
the \emph{world}
consists of a set of \emph{objects}, each of which may exhibit
some subset of the \emph{features} in the set of possible features.
The learner is aware of exactly which features each object exhibits.

Following a suggestion of \citet{rrdnf}, we attempt to extend their
work by employing ``strong sampling'':
A subset~$C$ of the objects belong to a \emph{concept}, which the learner
seeks to learn. A few objects are selected randomly from $C$, and
perhaps a few are also selected from the complement of $C$. These few
objects and an indication of whether each belongs to $C$ are given to
the learner (this is the \emph{observed data}). The learner receives
incorrect information about whether a particular object is in $C$ 
with probability $e^{-b}$, for some \emph{outlier parameter} $b$,
as in \citet{rrdnf}.

Given a few positive examples of a concept, there are potentially
many possible larger sets $C$ that contain those examples. How is the
learner to choose from among them? \citet{feldman} found that the most
natural concepts are the simple ones, that is, those with short
representations as boolean propositional formulas in terms of feature
predicates $f_i$, where $f_i(x)$ is true if and only if object $x$
exhibits feature~$i$. Such a formula might look like
\[ (f_1(x)\wedge f_3(x)) \vee f_2(x).  \]

Again following \citet{rrdnf}, we will consider formulas in
\emph{disjunctive normal form}. These formulas are disjunctions
of conjunctions of terms of the form $f_i(x)$ or $\neg f_i(x)$.
A grammar for such formulas is given by
\begin{align*}
	S &\to x\in C \Leftrightarrow (\Disj)\\
	\Disj &\to (\Conj) \vee \Disj \\
	\Disj &\to \False \\
	\Conj &\to P \wedge \Conj\\
	\Conj &\to \True\\
	P &\to f_1(x)\\
	P &\to \neg f_1(x)\\
	P &\to f_2(x)\\
	P &\to \neg f_2(x)\\
	&\vdots
\end{align*}
With the standard semantics, a formula generated by this grammar
picks out a specific set of objects --- a \emph{hypothesis}
for the concept.

Any partition of the objects into concepts and non-concepts
is specified by some formula, so long as objects with the same 
features are categorized in the same way. Two formulas may define the
same concept.

If the representation of learned concepts in the mind is
something like these formulas, then we are interested in the most
likely formula

Results are presented in Table~\ref{tab1}.
\begin{table}[!htb]
\caption{Sample table.}\label{tab1}
\begin{tabular}{ccc}
\hline\\[-1.5ex]
AAA & BBB & CCC \\[0.5ex]
\hline\\[-1.5ex]
1.0 & 2.0 & 3.0\\[0.5ex]
1.0 & 2.0 & 3.0\\[0.5ex]
\hline
\end{tabular}
\end{table}


\bibliography{report}

\end{document}

